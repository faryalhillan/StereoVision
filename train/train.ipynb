{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276cec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f7bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv11 model configuration\n",
    "model = YOLO(\"yolo11s.pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training arguments\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_dir = f\"runs/train/{timestamp}\"\n",
    "train_args = {\n",
    "    \"data\": \"../data/dataset/data.yaml\",\n",
    "    \"epochs\": 150,\n",
    "    \"imgsz\": 1280,\n",
    "    \"batch\": 8,\n",
    "    \"patience\": 50,\n",
    "    \"save\": True,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"save_dir\": save_dir,\n",
    "    \"lr0\": 0.01,\n",
    "    \"close_mosaic\": 20,\n",
    "    \"name\": f\"train_{timestamp}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81efcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on custom dataset\n",
    "results = model.train(**train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b91ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.val()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913269e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c0e3b4",
   "metadata": {},
   "source": [
    "## Export to TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8cb625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: LOAD model\n",
    "model = YOLO(\"runs/detect/720_best/weights/best.pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0582e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(\n",
    "    format=\"engine\",           \n",
    "    half=True,                \n",
    "    imgsz=1280,                    \n",
    "    batch=6,                   \n",
    "    dynamic=True,             \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdc4fd",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed9b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Paths to your models\n",
    "pt_path = \"runs/detect/720_best/weights/best.pt\"\n",
    "engine_path = \"runs/detect/720_best/weights/best.engine\"\n",
    "\n",
    "# Load models\n",
    "model_pt = YOLO(pt_path).to(\"cuda\")\n",
    "model_engine = YOLO(engine_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb2d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model, test_images, num_runs=100, warmup_runs=10):\n",
    "    \"\"\"\n",
    "    Benchmark model performance with detailed metrics\n",
    "    \"\"\"\n",
    "    # Warmup runs\n",
    "    for _ in range(warmup_runs):\n",
    "        _ = model(test_images[0])\n",
    "    \n",
    "    # Benchmark runs\n",
    "    inference_times = []\n",
    "    memory_usage_before = []\n",
    "    memory_usage_after = []\n",
    "    \n",
    "    import torch\n",
    "    import psutil\n",
    "    import gc\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        # Clear cache and collect garbage\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Record memory before inference\n",
    "        memory_usage_before.append(torch.cuda.memory_allocated() / 1024**2)  # MB\n",
    "        \n",
    "        # Time inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            results = model(test_images[i % len(test_images)])\n",
    "        torch.cuda.synchronize()  # Wait for GPU operations to complete\n",
    "        end_time = time.time()\n",
    "        \n",
    "        inference_times.append((end_time - start_time) * 1000)  # Convert to ms\n",
    "        \n",
    "        # Record memory after inference\n",
    "        memory_usage_after.append(torch.cuda.memory_allocated() / 1024**2)  # MB\n",
    "    \n",
    "    return {\n",
    "        'inference_times': inference_times,\n",
    "        'mean_time': np.mean(inference_times),\n",
    "        'std_time': np.std(inference_times),\n",
    "        'min_time': np.min(inference_times),\n",
    "        'max_time': np.max(inference_times),\n",
    "        'median_time': np.median(inference_times),\n",
    "        'fps': 1000 / np.mean(inference_times),\n",
    "        'memory_before': np.mean(memory_usage_before),\n",
    "        'memory_after': np.mean(memory_usage_after),\n",
    "        'memory_diff': np.mean(memory_usage_after) - np.mean(memory_usage_before)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bd7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test images for benchmarking\n",
    "import cv2\n",
    "import torch\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Get some test images from your dataset\n",
    "test_image_paths = glob.glob(\"../data/dataset/valid/images/*.jpg\")[:50]  # Use first 50 validation images\n",
    "if not test_image_paths:\n",
    "    test_image_paths = glob.glob(\"../data/dataset/valid/images/*.png\")[:50]\n",
    "\n",
    "print(f\"Found {len(test_image_paths)} test images\")\n",
    "\n",
    "# Load and preprocess test images\n",
    "test_images = []\n",
    "for img_path in test_image_paths[:20]:  # Use 20 images for testing\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is not None:\n",
    "        test_images.append(img_path)\n",
    "\n",
    "print(f\"Loaded {len(test_images)} test images for benchmarking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee152c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark PyTorch model (.pt)\n",
    "print(\"Benchmarking PyTorch model (.pt)...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "pt_results = benchmark_model(model_pt, test_images, num_runs=100, warmup_runs=10)\n",
    "\n",
    "print(f\"PyTorch Model Performance:\")\n",
    "print(f\"Mean inference time: {pt_results['mean_time']:.2f} ± {pt_results['std_time']:.2f} ms\")\n",
    "print(f\"Median inference time: {pt_results['median_time']:.2f} ms\")\n",
    "print(f\"Min inference time: {pt_results['min_time']:.2f} ms\")\n",
    "print(f\"Max inference time: {pt_results['max_time']:.2f} ms\")\n",
    "print(f\"Average FPS: {pt_results['fps']:.2f}\")\n",
    "print(f\"Memory usage: {pt_results['memory_before']:.2f} MB → {pt_results['memory_after']:.2f} MB\")\n",
    "print(f\"Memory difference per inference: {pt_results['memory_diff']:.2f} MB\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d8c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark TensorRT engine model (.engine)\n",
    "print(\"Benchmarking TensorRT engine model (.engine)...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "engine_results = benchmark_model(model_engine, test_images, num_runs=100, warmup_runs=10)\n",
    "\n",
    "print(f\"TensorRT Engine Model Performance:\")\n",
    "print(f\"Mean inference time: {engine_results['mean_time']:.2f} ± {engine_results['std_time']:.2f} ms\")\n",
    "print(f\"Median inference time: {engine_results['median_time']:.2f} ms\")\n",
    "print(f\"Min inference time: {engine_results['min_time']:.2f} ms\")\n",
    "print(f\"Max inference time: {engine_results['max_time']:.2f} ms\")\n",
    "print(f\"Average FPS: {engine_results['fps']:.2f}\")\n",
    "print(f\"Memory usage: {engine_results['memory_before']:.2f} MB → {engine_results['memory_after']:.2f} MB\")\n",
    "print(f\"Memory difference per inference: {engine_results['memory_diff']:.2f} MB\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0db75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Performance Comparison\n",
    "print(\"DETAILED PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Speed comparison\n",
    "speed_improvement = ((pt_results['mean_time'] - engine_results['mean_time']) / pt_results['mean_time']) * 100\n",
    "fps_improvement = ((engine_results['fps'] - pt_results['fps']) / pt_results['fps']) * 100\n",
    "\n",
    "print(f\"   INFERENCE SPEED COMPARISON:\")\n",
    "print(f\"   PyTorch (.pt):     {pt_results['mean_time']:.2f} ms/image ({pt_results['fps']:.2f} FPS)\")\n",
    "print(f\"   TensorRT (.engine): {engine_results['mean_time']:.2f} ms/image ({engine_results['fps']:.2f} FPS)\")\n",
    "print(f\"   Speed improvement:  {speed_improvement:.1f}% faster\")\n",
    "print(f\"   FPS improvement:    {fps_improvement:.1f}% higher\")\n",
    "print()\n",
    "\n",
    "# Memory comparison\n",
    "memory_improvement = ((pt_results['memory_diff'] - engine_results['memory_diff']) / abs(pt_results['memory_diff'])) * 100 if pt_results['memory_diff'] != 0 else 0\n",
    "\n",
    "print(f\"   MEMORY USAGE COMPARISON:\")\n",
    "print(f\"   PyTorch (.pt):     {pt_results['memory_diff']:.2f} MB per inference\")\n",
    "print(f\"   TensorRT (.engine): {engine_results['memory_diff']:.2f} MB per inference\")\n",
    "print(f\"   Memory efficiency:  {memory_improvement:.1f}% {'better' if memory_improvement > 0 else 'worse'}\")\n",
    "print()\n",
    "\n",
    "# Consistency comparison\n",
    "pt_cv = (pt_results['std_time'] / pt_results['mean_time']) * 100\n",
    "engine_cv = (engine_results['std_time'] / engine_results['mean_time']) * 100\n",
    "\n",
    "print(f\"   CONSISTENCY COMPARISON:\")\n",
    "print(f\"   PyTorch (.pt):     CV = {pt_cv:.2f}% (std: {pt_results['std_time']:.2f} ms)\")\n",
    "print(f\"   TensorRT (.engine): CV = {engine_cv:.2f}% (std: {engine_results['std_time']:.2f} ms)\")\n",
    "print(f\"   More consistent:    {'TensorRT' if engine_cv < pt_cv else 'PyTorch'}\")\n",
    "print()\n",
    "\n",
    "# Throughput analysis\n",
    "print(f\"   THROUGHPUT ANALYSIS:\")\n",
    "print(f\"   PyTorch theoretical max:  {pt_results['fps']:.0f} images/second\")\n",
    "print(f\"   TensorRT theoretical max: {engine_results['fps']:.0f} images/second\")\n",
    "print(f\"   Throughput gain:          {engine_results['fps'] - pt_results['fps']:.0f} additional images/second\")\n",
    "print()\n",
    "\n",
    "# Model size comparison (if files exist)\n",
    "import os\n",
    "if os.path.exists(pt_path) and os.path.exists(engine_path):\n",
    "    pt_size = os.path.getsize(pt_path) / (1024**2)  # MB\n",
    "    engine_size = os.path.getsize(engine_path) / (1024**2)  # MB\n",
    "    size_ratio = engine_size / pt_size\n",
    "    \n",
    "    print(f\"   MODEL SIZE COMPARISON:\")\n",
    "    print(f\"   PyTorch (.pt):     {pt_size:.1f} MB\")\n",
    "    print(f\"   TensorRT (.engine): {engine_size:.1f} MB\")\n",
    "    print(f\"   Size ratio:        {size_ratio:.2f}x {'larger' if size_ratio > 1 else 'smaller'}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Performance Comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Inference Time Distribution\n",
    "axes[0, 0].hist(pt_results['inference_times'], bins=30, alpha=0.7, label='PyTorch (.pt)', color='blue')\n",
    "axes[0, 0].hist(engine_results['inference_times'], bins=30, alpha=0.7, label='TensorRT (.engine)', color='red')\n",
    "axes[0, 0].set_xlabel('Inference Time (ms)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Inference Time Distribution')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Performance Metrics Comparison\n",
    "metrics = ['Mean Time (ms)', 'FPS', 'Memory (MB)']\n",
    "pt_values = [pt_results['mean_time'], pt_results['fps'], pt_results['memory_diff']]\n",
    "engine_values = [engine_results['mean_time'], engine_results['fps'], engine_results['memory_diff']]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0, 1].bar(x - width/2, pt_values, width, label='PyTorch (.pt)', color='blue', alpha=0.7)\n",
    "bars2 = axes[0, 1].bar(x + width/2, engine_values, width, label='TensorRT (.engine)', color='red', alpha=0.7)\n",
    "\n",
    "axes[0, 1].set_xlabel('Metrics')\n",
    "axes[0, 1].set_ylabel('Values')\n",
    "axes[0, 1].set_title('Performance Metrics Comparison')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(metrics, rotation=45)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[0, 1].annotate(f'{height:.1f}',\n",
    "                           xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                           xytext=(0, 3),  # 3 points vertical offset\n",
    "                           textcoords=\"offset points\",\n",
    "                           ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 3. Time Series Comparison (First 50 runs)\n",
    "runs = np.arange(1, min(51, len(pt_results['inference_times']) + 1))\n",
    "axes[1, 0].plot(runs, pt_results['inference_times'][:50], 'b-', alpha=0.7, label='PyTorch (.pt)')\n",
    "axes[1, 0].plot(runs, engine_results['inference_times'][:50], 'r-', alpha=0.7, label='TensorRT (.engine)')\n",
    "axes[1, 0].set_xlabel('Run Number')\n",
    "axes[1, 0].set_ylabel('Inference Time (ms)')\n",
    "axes[1, 0].set_title('Inference Time Over Runs (First 50)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Box Plot Comparison\n",
    "data_to_plot = [pt_results['inference_times'], engine_results['inference_times']]\n",
    "bp = axes[1, 1].boxplot(data_to_plot, labels=['PyTorch (.pt)', 'TensorRT (.engine)'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('blue')\n",
    "bp['boxes'][0].set_alpha(0.7)\n",
    "bp['boxes'][1].set_facecolor('red')\n",
    "bp['boxes'][1].set_alpha(0.7)\n",
    "axes[1, 1].set_ylabel('Inference Time (ms)')\n",
    "axes[1, 1].set_title('Inference Time Distribution (Box Plot)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
